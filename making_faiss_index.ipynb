{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('title_url_text.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_metadata(data, default_metadata):\n",
    "    default_metadata['title'] = data['title']\n",
    "    default_metadata['url'] = data['url']\n",
    "    return default_metadata\n",
    "\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='title_url_text.json',\n",
    "    # file_path='title_url_text_test.json',\n",
    "    jq_schema='.[]',\n",
    "    content_key='text',\n",
    "    metadata_func=make_metadata,\n",
    "    text_content=True,\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3699"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(data)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n",
      "332\n",
      "397\n"
     ]
    }
   ],
   "source": [
    "# num of words in chunks\n",
    "for doc in docs[:3]:\n",
    "    print(len(doc.page_content.replace('.', ' ').split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"perfect goals of \\'provable\\' alignment, nor total alignment of superintelligences on exact human values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_scores = new_db.similarity_search(query, k=10)\n",
    "len(docs_and_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2\\n-2.  When I say that alignment is lethally difficult, I am not talking about ideal or perfect goals of \\'provable\\' alignment, nor total alignment of superintelligences on exact human values, nor getting AIs to produce satisfactory arguments about moral dilemmas which sorta-reasonable humans disagree about, nor attaining an absolute certainty of an AI not killing everyone.  When I say that alignment is difficult, I mean that in practice, using the techniques we actually have, \"please don\\'t disassemble literally everyone with probability roughly 1\" is an overly large ask that we are not on course to get.  So far as I\\'m concerned, if you can get a powerful AGI that carries out some pivotal superhuman engineering task, with a less than fifty percent change of killing more than one billion people, I\\'ll take it.  Even smaller chances of killing even fewer people would be a nice luxury, but if you can get as incredibly far as \"less than roughly certain to kill everybody\", then you can probably get down to under a 5% chance with only slightly more effort.  Practically all of the difficulty is in getting to \"less than certainty of killing literally everyone\".  Trolley problems are not an interesting subproblem in all of this; if there are any survivors, you solved alignment.  At this point, I no longer care how it works, I don\\'t care how you got there, I am cause-agnostic about whatever methodology you used, all I am looking at is prospective results, all I want is that we have justifiable cause to believe of a pivotally useful AGI \\'this will not kill literally everyone\\'.  Anybody telling you I\\'m asking for stricter \\'alignment\\' than this has failed at reading comprehension.  The big ask from AGI alignment, the basic challenge I am saying is too difficult, is to obtain by any strategy whatsoever a significant chance of there being any survivors.', metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 1, 'title': 'AGI Ruin: A List of Lethalities', 'url': 'https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'}),\n",
       " Document(page_content='22.  There\\'s a relatively simple core structure that explains why complicated cognitive machines work; which is why such a thing as general intelligence exists and not just a lot of unrelated special-purpose solutions; which is why capabilities generalize after outer optimization infuses them into something that has been optimized enough to become a powerful inner optimizer.  The fact that this core structure is simple and relates generically to low-entropy high-structure environments is why humans can walk on the Moon.  There is no analogous truth about there being a simple core of alignment, especially not one that is even easier for gradient descent to find than it would have been for natural selection to just find \\'want inclusive reproductive fitness\\' as a well-generalizing solution within ancestral humans.  Therefore, capabilities generalize further out-of-distribution than alignment, once they start to generalize at all.\\n23.  Corrigibility is anti-natural to consequentialist reasoning; \"you can\\'t bring the coffee if you\\'re dead\" for almost every kind of coffee.  We (MIRI) tried and failed to find a coherent formula for an agent that would let itself be shut down (without that agent actively trying to get shut down).  Furthermore, many anti-corrigible lines of reasoning like this may only first appear at high levels of intelligence.\\n24.  There are two fundamentally different approaches you can potentially take to alignment, which are unsolvable for two different sets of reasons; therefore, by becoming confused and ambiguating between the two approaches, you can confuse yourself about whether alignment is necessarily difficult.  The first approach is to build a CEV-style Sovereign which wants exactly what we extrapolated-want and is therefore safe to let optimize all the future galaxies without it accepting any human input trying to stop it.  The second course is to build corrigible AGI which doesn\\'t want exactly what we want, and yet somehow fails to kill us and take over the galaxies despite that being a convergent incentive there.', metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 1, 'title': 'AGI Ruin: A List of Lethalities', 'url': 'https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'}),\n",
       " Document(page_content=\"Section B.2:  Central difficulties of outer and inner alignment. \\n16.  Even if you train really hard on an exact loss function, that doesn't thereby create an explicit internal representation of the loss function inside an AI that then continues to pursue that exact loss function in distribution-shifted environments.  Humans don't explicitly pursue inclusive genetic fitness; outer optimization even on a very exact, very simple loss function doesn't produce inner optimization in that direction.  This happens in practice in real life, it is what happened in the only case we know about, and it seems to me that there are deep theoretical reasons to expect it to happen again: the first semi-outer-aligned solutions found, in the search ordering of a real-world bounded optimization process, are not inner-aligned solutions.  This is sufficient on its own, even ignoring many other items on this list, to trash entire categories of naive alignment proposals which assume that if you optimize a bunch on a loss function calculated using some simple concept, you get perfect inner alignment on that concept.\\n17.  More generally, a superproblem of 'outer optimization doesn't produce inner alignment' is that on the current optimization paradigm there is no general idea of how to get particular inner properties into a system, or verify that they're there, rather than just observable outer ones you can run a loss function over.  This is a problem when you're trying to generalize out of the original training distribution, because, eg, the outer behaviors you see could have been produced by an inner-misaligned system that is deliberately producing outer behaviors that will fool you.  We don't know how to get any bits of information into the inner system rather than the outer behaviors, in any systematic or general way, on the current optimization paradigm.\", metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 1, 'title': 'AGI Ruin: A List of Lethalities', 'url': 'https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'}),\n",
       " Document(page_content='In particular, just as I have a model of the Other Person\\'s Beliefs in which they think alignment is easy because they don\\'t know about difficulties I see as very deep and fundamental and hard to avoid, I also have a model in which people think \"why not just build an AI which does X but not Y?\" because they don\\'t realize what X and Y have in common, which is something that draws deeply on having deep models of intelligence. And it is hard to convey this deep theoretical grasp.\\nBut you can also see powerful practical hints that these things are much more correlated than, eg, Robin Hanson was imagining during the FOOM debate, because Robin did not think something like GPT-3 should exist; Robin thought you should need to train lots of specific domains that didn\\'t generalize. I argued then with Robin that it was something of a hint that humans had visual cortex and cerebellar cortex but not Car Design Cortex, in order to design cars. Then in real life, it proved that reality was far to the Eliezer side of Eliezer on the Eliezer-Robin axis, and things like GPT-3 were built with less architectural complexity and generalized more than I was arguing to Robin that complex architectures should generalize over domains.\\nThe metaphor I sometimes use is that it is very hard to build a system that drives cars painted red, but is not at all adjacent to a system that could, with a few alterations, prove to be very good at driving a car painted blue.  The \"drive a red car\" problem and the \"drive a blue car\" problem have too much in common.  You can maybe ask, \"Align a system so that it has the capability to drive red cars, but refuses to drive blue cars.\"  You can\\'t make a system that is very good at driving red-painted cars, but lacks the basic capability to drive blue-painted cars because you never trained it on that.  The patterns found by gradient descent, by genetic algorithms, or by other plausible methods of optimization, for driving red cars, would be patterns very close to the ones needed to drive blue cars.  When you optimize for red cars you get the blue car capability whether you like it or not.\\n[Ngo][11:32] \\nDoes your model of intelligence rule out building AIs which make dramatic progress in mathematics without killing us all?', metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 147, 'title': 'Ngo and Yudkowsky on alignment difficulty', 'url': 'https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty'}),\n",
       " Document(page_content=\"maude:  And your claim is that no one else in the world is smart enough to notice any of this?\\neliezer:  No, that's not what I'm saying. Concerns like ‚Äúhow do we specify correct goals for par-human AI?‚Äù and ‚Äúwhat happens when AI gets smart enough to automate AI research itself?‚Äù have been around for a long time, sort of just hanging out and not visibly shifting research priorities. So it's not that the community of people who have ever thought about superintelligence is small; and it's not that there are no ongoing lines of work on robustness, transparency, or security in narrow AI systems that will incidentally make it easier to align smarter-than-human AI. But the community of people who go into work every day and make decisions about what technical problems to tackle based on any extended thinking related to superintelligent AI is very small.\\nmaude:  What I‚Äôm saying is that you‚Äôre jumping ahead and trying to solve the far end of the problem before the field is ready to focus efforts there. The current work may not all bear directly on superintelligence, but we should expect all the significant progress on AI alignment to be produced by the intellectual heirs of the people presently working on topics like drone warfare and unemployment.\\npat:  (cautiously)  I mean, if what Eliezer says is true‚Äîand I do think that Eliezer is honest, if often, by my standards, slightly crazy‚Äîthen the state of the field in 2010 is just like it looks naively. There aren‚Äôt many people working on topics related to smarter-than-human AI, and Eliezer‚Äôs group and the Oxford Future of Humanity Institute are the only ones with a reasonable claim to be working on AI alignment. If Eliezer says that the problems of crafting a smarter-than-human AI to not kill everyone are not of a type with current machine ethics work, then I can buy that as plausible, though I‚Äôd want to hear others‚Äô views on the issue before reaching a firm conclusion.\\nmaude:  But Eliezer‚Äôs field of competition is far wider than just the people writing ethics papers. Anyone working in machine learning, or indeed in any branch of computer science, might end up contributing to AI alignment.\", metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 280, 'title': 'Hero Licensing', 'url': 'https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing'}),\n",
       " Document(page_content='32.  Human thought partially exposes only a partially scrutable outer surface layer.  Words only trace our real thoughts.  Words are not an AGI-complete data representation in its native style.  The underparts of human thought are not exposed for direct imitation learning and can\\'t be put in any dataset.  This makes it hard and probably impossible to train a powerful system entirely on imitation of human words or other human-legible contents, which are only impoverished subsystems of human thoughts; unless that system is powerful enough to contain inner intelligences figuring out the humans, and at that point it is no longer really working as imitative human thought.\\n33.  The AI does not think like you do, the AI doesn\\'t have thoughts built up from the same concepts you use, it is utterly alien on a staggering scale.  Nobody knows what the hell GPT-3 is thinking, not only because the matrices are opaque, but because the stuff within that opaque container is, very likely, incredibly alien - nothing that would translate well into comprehensible human thinking, even if we could see past the giant wall of floating-point numbers to what lay behind.\\n Section B.4:  Miscellaneous unworkable schemes. \\n34.  Coordination schemes between superintelligences are not things that humans can participate in (eg because humans can\\'t reason reliably about the code of superintelligences); a \"multipolar\" system of 20 superintelligences with different utility functions, plus humanity, has a natural and obvious equilibrium which looks like \"the 20 superintelligences cooperate with each other but not with humanity\".', metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 1, 'title': 'AGI Ruin: A List of Lethalities', 'url': 'https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'}),\n",
       " Document(page_content='During this time period, leakage of the code to the wider world automatically results in the world being turned into paperclips.  Leakage of the code to multiple major actors such as commercial espionage groups or state intelligence agencies seems to me to stand an extremely good chance of destroying the world because at least one such state actor\\'s command will not reprise the alignment debate correctly and each of them will fear the others.\\nI would also expect that, if key ideas and architectural lessons-learned were to leak from an insufficiently closed project that would otherwise have actually developed alignable AGI, it would be possible to use 10% as much labor to implement a non-alignable world-destroying AGI in a shorter timeframe.  The project must be closed tightly or everything ends up as paperclips.\\n\"Adequacy\" on common good commitment is based on my model wherein the first task-directed AGI continues to operate in a regime far below that of a real superintelligence, where many tradeoffs have been made for transparency over capability and this greatly constrains self-modification.\\nThis task-directed AGI is not able to defend against true superintelligent attack.  It cannot monitor other AGI projects in an unobtrusive way that grants those other AGI projects a lot of independent freedom to do task-AGI-ish things so long as they don\\'t create an unrestricted superintelligence.  The designers of the first task-directed AGI are barely able to operate it in a regime where the AGI doesn\\'t create an unaligned superintelligence inside itself or its environment.  Safe operation of the original AGI requires a continuing major effort at supervision.  The level of safety monitoring of other AGI projects required would be so great that, if the original operators deemed it good that more things be done with AGI powers, it would be far simpler and safer to do them as additional tasks running on the original task-directed AGI.  Therefore:  Everything to do with invocation of superhuman specialized general intelligence, like superhuman science and engineering, continues to have a single effective veto point.', metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 58, 'title': 'Six Dimensions of Operational Adequacy in AGI Projects', 'url': 'https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects'}),\n",
       " Document(page_content=\"Two arguments I can see for the second sense: (1) the non-superintelligences only seem to respond well to alignment schemes because they don't yet have the core of general intelligence, and (2) the non-superintelligences only seem to respond well to alignment schemes because despite being misaligned they are doing what we want in order to survive and later execute a treacherous turn. EDIT: And (3) fast takeoff = not much time to look at the closest non-dangerous examples\\n(I still should sleep, but would be interested in seeing thoughts tomorrow, and if enough people think it's actually worthwhile to engage on the meta level I can do that. I'm cheerful about engaging on specific object-level ideas.)\\n[Soares: üí§]\\n[Yudkowsky][15:28] \\nit's not that early failures tell you nothing\\nthe failure of the 1955 Dartmouth Project to produce strong AI over a summer told those researchers something\\nit told them the problem was harder than they'd hoped on the first shot\\nit didn't show them the correct way to build AGI in 1957 instead\\n[Bensinger][16:41] \\nLinking to a chat log between Eliezer and some anonymous people (and Steve Omohundro) from early September: [https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions]\\nEliezer tells me he thinks it pokes at some of Rohin's questions\\n[Yudkowsky][16:48] \\nI'm not sure that I can successfully, at this point, go back up and usefully reply to the text that scrolled past - I also note some internal grinding about this having turned into a thing which has Pending Replies instead of Scheduled Work Hours - and this maybe means that in the future we shouldn't have such a general chat here, which I didn't anticipate before the fact.  I shall nonetheless try to pick out some things and reply to them.\\n[Shah: üëç]\", metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 195, 'title': 'Shah and Yudkowsky on alignment failures', 'url': 'https://www.lesswrong.com/posts/tcCxPLBrEXdxN5HCQ/shah-and-yudkowsky-on-alignment-failures'}),\n",
       " Document(page_content=\"eliezer:  Let me guess. You don‚Äôt think I can do that either.\\npat:  Well, I don‚Äôt think you can save the world, of course!  (laughs)  This isn‚Äôt a science fiction book. But I do see how you can reasonably hope to make an important contribution to the theory of Friendly AI that ends up being useful to whatever group ends up developing general AI. What‚Äôs interesting to note here is that the scenario the Masked Stranger described, the class of successes you assigned 10% aggregate probability, is actually harder to achieve than that.\\nstranger:  (smiling)  It really, really, really isn‚Äôt.\\nI'll mention as an aside that talk of ‚ÄúFriendly‚Äù AI has been going out of style where I‚Äôm from. We‚Äôve started talking instead in terms of ‚Äúaligning smarter-than-human AI with operators‚Äô goals,‚Äù mostly because ‚ÄúAI alignment‚Äù smacks less of anthropomorphism than ‚Äúfriendliness.‚Äù\\neliezer:  Alignment? Okay, I can work with that. But Pat, you‚Äôve said something I didn‚Äôt expect you to say and gone outside my current vision of your Ideological Turing Test. Please continue.\\npat:  Okay. Contrary to what you think, my words are not fully general counterarguments that I launch against just anything I intuitively dislike. They are based on specific, visible, third-party-assessable factors that make assertions believable or unbelievable. If we leave aside inaccessible intuitions and just look at third-party-visible factors, then it is very clear that there‚Äôs a huge community of writers who are explicitly trying to create Harry Potter fanfiction. This community is far larger and has far more activity‚Äîby every objective, third-party metric‚Äîthan the community working on issues related to alignment or friendliness or whatever. Being the best writer in a much larger community is much more improbable than your making a significant contribution to AI alignment when almost nobody else is working on that problem.\", metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 280, 'title': 'Hero Licensing', 'url': 'https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing'}),\n",
       " Document(page_content='[Alexander: üëç]\\n 3. Acausal trade, and alignment research opportunities\\n[Alexander][17:13]   \\nAll right. I want to try one more theoretical question before moving on to a hopefully much shorter practical question. And by \"theoretical question\" I mean \"desperate grasping at emotional straws\". Consider the following scenarios:\\n1. An unaligned superintelligence decides whether or not to destroy humanity. If Robin Hanson\\'s \"grabby alien\" model is true, it expects to one day meet alien superintelligences and split the universe with them. Some of these aliens might have successfully aligned their AGIs, and they might do some kind of acausal bargaining where their AGI is nicer to other AGIs who leave their creator species with at least one planet/galaxy whatever, in exchange for us trying the same if we succeed. Given the superintelligence\\'s reasonable expectation of millions of planets/galaxies, it might decide that even this small chance is worth sacrificing one of them for, and give humans some trivial (from its perspective) concession (which might still look like an amazing utopia from our perspective).\\n2. Some version of the simulation argument plus Stuart Armstrong\\'s \"the AI in the box boxes you\". The unaligned superintelligence considers whether some species who successfully aligned AI might run a billion simulations of slightly different AI scenarios and give the ones who are nice to their creators some big reward. Given that it\\'s anthropically more likely that this happened than that they\\'re really the single first superintelligence ever, it agrees to give us some trivial concession which looks like amazing utopia to us.\\nAre either of these plausible? If so, is there anything we can do now to encourage them? If (crazy example), the UN passes a resolution saying it will definitely do something like this if we align AI correctly, does that change the calculus somehow?\\n[Yudkowsky][17:17]', metadata={'source': '/Users/yaroslav.poltoran/Documents/experiments/test_shit/lesswrong/title_url_text.json', 'seq_num': 479, 'title': 'Alexander and Yudkowsky on AGI goals', 'url': 'https://www.lesswrong.com/posts/rwkkcgSpnAyE8oNo3/alexander-and-yudkowsky-on-agi-goals'})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube_sum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
